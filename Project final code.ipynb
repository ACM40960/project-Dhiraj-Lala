{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae398b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Dhiraj/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "C:\\Users\\Dhiraj\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "WARNING  'ultralytics.yolo.v8' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.models.yolo' instead.\n",
      "WARNING  'ultralytics.yolo.utils' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.utils' instead.\n",
      "Note this warning may be related to loading older models. You can update your model to current structure with:\n",
      "    import torch\n",
      "    ckpt = torch.load(\"model.pt\")  # applies to both official and custom models\n",
      "    torch.save(ckpt, \"updated-model.pt\")\n",
      "\n",
      "YOLOv5  2023-7-19 Python-3.9.12 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "\n",
    "# Load the YOLOv5m model\n",
    "model_path = 'yolov5m.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "yolo_model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5m\")\n",
    "yolo_model.to(device)\n",
    "yolo_model.eval()# Set the YOLO model to evaluation mode for faster inference\n",
    "frame_processing_times = []\n",
    "detected_object_counts = []\n",
    "tracked_object_counts = []\n",
    "\n",
    "# Loading the video and converting it to .mp4v\n",
    "path_input_video = cv2.VideoCapture('C:/Users/Dhiraj/Downloads/bdd100k_videos_train_00/bdd100k/videos/train/002ab96a-ea678692.mov')\n",
    "path_output_video = 'C:/Users/Dhiraj/Downloads/bdd100k_videos_train_00/bdd100k/videos/train/002ab96a-ea678692.mp4'\n",
    "width, height = 1280, 720\n",
    "Output_Video = cv2.VideoWriter(path_output_video, cv2.VideoWriter_fourcc(*'mp4v'), 20, (width, height))\n",
    "enable_track = False\n",
    "\n",
    "#class\n",
    "yolo_classes = [\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella'\n",
    "]\n",
    "\n",
    "\n",
    "def transparent_overlay_check(Overlay_background, Foreground_overlay, angle, x, y, objSize=50):\n",
    "    original_frame = Overlay_background.copy()\n",
    "    Foreground_overlay = cv2.resize(Foreground_overlay, (objSize, objSize))\n",
    "\n",
    "\n",
    "    # Extract the dimensions and channel count of the foreground\n",
    "    check_rows, check_cols, channels = Foreground_overlay.shape\n",
    "\n",
    "    # Determine the midpoint of the foreground\n",
    "    center_x = int(check_cols / 2)\n",
    "    center_y = int(check_rows / 2)\n",
    "\n",
    "    # Generate the rotation matrix for the desired angle\n",
    "    M = cv2.getRotationMatrix2D((center_x, center_y), angle, 1)\n",
    "    \n",
    "    # Apply the rotation transformation to the foreground\n",
    "    Foreground_overlay = cv2.warpAffine(Foreground_overlay, M, (check_cols, check_rows))\n",
    "\n",
    "    # Merge the rotated foreground with the background based on alpha values\n",
    "    for row in range(check_rows):\n",
    "        for col in range(check_cols):\n",
    "            if x + row < Overlay_background.shape[0] and y + col < Overlay_background.shape[1]:\n",
    "                alpha = Foreground_overlay[row, col, 3] / 255.0\n",
    "                Overlay_background[x + row, y + col] = alpha * Foreground_overlay[row, col, :3] + (1 - alpha) * Overlay_background[x + row, y + col]\n",
    "\n",
    "    # Return the combined image\n",
    "    result = Overlay_background\n",
    "    return result\n",
    "\n",
    "\n",
    "def simulation_of_object(Overlay_background, object_class, x, y):\n",
    "    # Retrieve the object image based on the given class\n",
    "    image_of_object = cv2.imread(f'assets/{object_class}.png', cv2.IMREAD_UNCHANGED)\n",
    "    if image_of_object is None:\n",
    "        return Overlay_background\n",
    "    # Merge the object image with the background at specified coordinates\n",
    "    Overlay_background[y:y+100, x:x+100] = transparent_overlay_check(Overlay_background[y:y+100, x:x+100], image_of_object, 0, 0, 0)\n",
    "    return Overlay_background\n",
    "\n",
    "def overlay_car(Overlay_background):\n",
    "    \n",
    "    # Load the car overlay image\n",
    "    overlay_img = cv2.imread('C:/Users/Dhiraj/Downloads/MyCar.png', cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "    # Determine overlay image dimensions\n",
    "    check_rows, check_cols, _ = overlay_img.shape\n",
    "    x = 550\n",
    "    y = Overlay_background.shape[0] - 200\n",
    "    \n",
    "    # Merge the car overlay with the background at the specified location\n",
    "    overlay_img = transparent_overlay_check(Overlay_background[y:y+check_rows, x:x+check_cols], overlay_img, 0, 0, 0, objSize=250)\n",
    "    Overlay_background[y:y+check_rows, x:x+check_cols] = overlay_img\n",
    "\n",
    "    return Overlay_background\n",
    "\n",
    "\n",
    "def object_bev_plot(transformed_image_with_centroids, source_points ,destination_points , objs_):\n",
    "    \n",
    "    # Compute perspective transformation matrix\n",
    "    M = cv2.getPerspectiveTransform(source_points, destination_points)\n",
    "    persObjs = []\n",
    "    \n",
    "    # Process each object's coordinates and ID\n",
    "    for obj_ in objs_:\n",
    "        if obj_:\n",
    "            # Convert centroid to numpy format\n",
    "            centroid_coordinates = np.array([list(obj_[0])], dtype=np.float32)\n",
    "\n",
    "            # Transform the centroid coordinates to new perspective\n",
    "            transformed_coordinates = cv2.perspectiveTransform(centroid_coordinates.reshape(-1, 1, 2), M)\n",
    "            transformed_coordinates_ = tuple(transformed_coordinates[0][0].astype(int))                                                       \n",
    "\n",
    "            # Mark the transformed centroid with a circle and annotate with class information\n",
    "            cv2.circle(transformed_image_with_centroids, transformed_coordinates_, radius=3, color=(0, 255, 0), thickness=-1)\n",
    "            cv2.circle(transformed_image_with_centroids, transformed_coordinates_, radius=12, color=(255, 255, 255), thickness=1)\n",
    "            class_text = f\"Class: {obj_[1]}\"\n",
    "            cv2.putText(transformed_image_with_centroids, class_text, (transformed_coordinates_[0] + 10, transformed_coordinates_[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "            persObjs.append([transformed_coordinates_, obj_[1]])\n",
    "\n",
    "                                                          \n",
    "                                                                                                                  \n",
    "    return transformed_image_with_centroids, persObjs\n",
    "                                      \n",
    "                                                                                                                                                                     \n",
    "# Pair current objects with their closest counterparts from the previous frame using Euclidean distance\n",
    "def tracking_and_distance_details(objects_curr, objects_prev, threshold=50):\n",
    "    object_tracking = {}\n",
    "    id_tracking = 0\n",
    "    for obj1 in objects_curr:\n",
    "        for obj2 in objects_prev:\n",
    "            dist = math.hypot(obj2[0][0] - obj1[0][0], obj2[0][1] - obj1[1])\n",
    "            if dist < threshold:\n",
    "                object_tracking[id_tracking] = obj1\n",
    "                id_tracking += 1\n",
    "    return object_tracking\n",
    "\n",
    "# Set inference batch size\n",
    "size_of_batch = 4\n",
    "frames = []\n",
    "frame_count = 0\n",
    "centroid_prev_frame = []\n",
    "object_tracking = {}\n",
    "id_tracking = 0\n",
    "\n",
    "\n",
    "# Iteratively process the video frames\n",
    "while True:\n",
    "    # Capture the next frame from video input\n",
    "    success, frame = path_input_video.read()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Terminate loop if end of video is reached\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Resize the captured frame and add to batch\n",
    "    if frame is not None:\n",
    "        frame = cv2.resize(frame, (width, height))\n",
    "        frames.append(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "        # Infer on the batch if it's full\n",
    "        if len(frames) == size_of_batch:\n",
    "            results = yolo_model(frames, size=320)\n",
    "\n",
    "            for idx, frame in enumerate(frames):\n",
    "                detections = results.pred[idx]\n",
    "\n",
    "                # Filter out detections below confidence threshold and apply non-max suppression\n",
    "                detections = detections[detections[:, 4] >= 0.3]  # Apply score threshold\n",
    "                detections = detections[torchvision.ops.nms(detections[:, :4], detections[:, 4], iou_threshold=0.5)]  # Apply non-max suppression\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Detect objects in the current frame\n",
    "    results = yolo_model(frame, size=160)\n",
    "    detections = results.pred[0]\n",
    "    # Initialize black images to overlay visualizations\n",
    "    image_ = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    simulated_image = image_.copy()\n",
    "    transformed_image_with_centroids = image_.copy()\n",
    "    transformed_image_to_sim = image_.copy()\n",
    "    simObjs = image_.copy()\n",
    "    objs = []\n",
    "    centroid_curr_frame = []\n",
    "\n",
    "    # Loop through detections to draw bounding boxes and gather object centroids                       \n",
    "    for detect in detections:\n",
    "        xmin = detect[0]\n",
    "        ymin = detect[1]\n",
    "        xmax = detect[2]\n",
    "        ymax = detect[3]\n",
    "        score = detect[4]\n",
    "        class_id = detect[5]\n",
    "        centroid_x = int(xmin + xmax) // 2\n",
    "        centroid_y = int(ymin + ymax) // 2\n",
    "\n",
    "        # Filter detections based on class ID and score\n",
    "        if int(class_id) in [0, 1, 2, 3, 5, 7] and score >= 0.3:\n",
    "            # Annotate detected objects with bounding boxes and labels\n",
    "            color = (0, 0, 255)\n",
    "            object_label = f\"{yolo_classes[int(class_id)]}: {score:.2f}\"\n",
    "            cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), color, 2)\n",
    "            cv2.putText(frame, object_label, (int(xmin), int(ymin) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 1)\n",
    "            # Store object centroids and class labels\n",
    "            centroid_curr_frame.append([(centroid_x, centroid_y), yolo_classes[int(class_id)]])\n",
    "            if not enable_track:\n",
    "                objs.append([(centroid_x, centroid_y), yolo_classes[int(class_id)]])\n",
    "    \n",
    "    # Count detected and tracked objects\n",
    "    detected_count = len(detections)\n",
    "    tracked_count = len(object_tracking)\n",
    "    detected_object_counts.append(detected_count)\n",
    "    tracked_object_counts.append(tracked_count)\n",
    "\n",
    "    \n",
    "    # Handle object tracking using centroids from the current and previous frames                        \n",
    "    if enable_track:\n",
    "        # Associate objects from current and previous frames if frame count is low\n",
    "        if frame_count <= 2:\n",
    "            for point_1, class_id in centroid_curr_frame:\n",
    "                for point_2, class_id in centroid_prev_frame:\n",
    "                    dist = math.hypot(point_2[0] - point_1[0], point_2[1] - point_1[1])\n",
    "                    if dist < 50:\n",
    "                        object_tracking[id_tracking] = point_1, class_id\n",
    "                        id_tracking += 1\n",
    "        # For subsequent frames, update or remove tracked objects based on their proximity                \n",
    "        else:\n",
    "            object_tracking_copy = object_tracking.copy()\n",
    "            for obj_id, point_2 in object_tracking_copy.items():\n",
    "                object_exists = False\n",
    "                for point_1, class_id in centroid_curr_frame:\n",
    "                    dist = math.hypot(point_2[0][0] - point_1[0], point_2[0][1] - point_1[1])\n",
    "                    if dist < 20:\n",
    "                        object_tracking[obj_id] = point_1, class_id\n",
    "                        object_exists = True\n",
    "                        continue\n",
    "                if not object_exists:\n",
    "                    object_tracking.pop(obj_id)\n",
    "\n",
    "        # Mark the tracked objects on the frame\n",
    "        for obj_id, point_1 in object_tracking.items():\n",
    "            cv2.circle(frame, point_1[0], 3, (0, 255, 255), -1)\n",
    "                                                                                                                      \n",
    "            if enable_track:\n",
    "                objs.append([point_1[0], point_1[1]])\n",
    "        # Store current frame's centroids for the next iteration\n",
    "        centroid_prev_frame = centroid_curr_frame.copy()\n",
    "\n",
    "    # Setting up the Bird's Eye View Transformation\n",
    "    # Define source points in the original frame\n",
    "    source_points = np.float32([(10, 720), (530, 400), (840, 400), (1270, 720)])\n",
    "    # Define destination points for the bird's eye view\n",
    "    destination_points = np.float32([(370, 720), (150, 0), (1130, 0), (900, 720)])\n",
    "    \n",
    "    # Transform and plot objects on the bird's eye view perspective\n",
    "    transformed_image_with_centroids, persObjs_ = object_bev_plot(transformed_image_with_centroids, source_points ,destination_points , objs)\n",
    "\n",
    "    # Simulate detected objects in the bird's eye view\n",
    "    for persObj_ in persObjs_:\n",
    "        simObjs = simulation_of_object(transformed_image_to_sim, persObj_[1], persObj_[0][0], persObj_[0][1])\n",
    "\n",
    "    # Overlay a car image onto the simulated objects frame\n",
    "    simulated_image = overlay_car(simObjs)\n",
    "\n",
    "    # Write the simulated image to the output video\n",
    "    Output_Video.write(simulated_image)\n",
    "\n",
    "    # Display the current frame, simulated objects, and the transformed frame\n",
    "    cv2.imshow(\"Original Frame\", frame)\n",
    "    cv2.imshow(\"Simulated Bird's Eye View\", simulated_image)\n",
    "    cv2.imshow(\"Transformed Bird's Eye View\", transformed_image_with_centroids)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    frame_processing_times.append(processing_time)\n",
    "\n",
    "\n",
    "    # Exit video loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Plot processing time per frame\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(frame_processing_times, marker='o')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Processing Time (seconds)')\n",
    "plt.title('Frame Processing Time')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot detected and tracked object counts\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(detected_object_counts, marker='o', label='Detected Objects')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Object Count')\n",
    "plt.title('Detected Object Counts')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Cleanup: release video input and output handles and close all OpenCV windows\n",
    "path_input_video.release()\n",
    "Output_Video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b799276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b8c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06009fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
